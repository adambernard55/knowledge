
## Specific Models Contents

## Contents

- **[[10_embeddinggemma|EmbeddingGemma: Technical Deep Dive on Embedding vs. Generative Models]]** — This document provides a technical breakdown of Google's EmbeddingGemma, using it as a prime example to explain the fundamental architectural and functional differences between embedding models and large-scale generative models. It details how embedding models convert text into numerical vectors for semantic search and RAG, contrasting this with the text-generation capabilities of models like ChatGPT. The note provides clear implementation logic for using these models in a typical RAG pipeline.
- **[[11_tiny-recursive-model-trm|TRM: Technical Deep Dive on Recursive vs. Autoregressive Models]]** — This document provides a technical analysis of Samsung's Tiny Recursive Model (TRM), a 7-million-parameter model. It explains how TRM's recursive, iterative refinement architecture allows it to outperform massive autoregressive LLMs (like Gemini and DeepSeek) on specific abstract reasoning benchmarks such as ARC-AGI. The note contrasts the 'decision-then-revision' process of TRM with the token-by-token generation of standard LLMs, highlighting the trade-off between parameter scale and test-time computational depth.
- **[[1_chatgpt|ChatGPT: Technical Deep Dive on GPT-4o vs. o1]]** — This document provides a detailed technical comparison between OpenAI's two flagship model lines: the multimodal GPT-4o and the reasoning-focused o1-series. It analyzes their core architectural differences, such as direct-answer vs. Chain-of-Thought logic, and provides performance benchmarks on speed, output capacity, and hallucination rates. The note concludes with specific implementation logic for technical teams to select the optimal model for tasks ranging from creative SEO to complex code architecture.
- **[[2_gemini|Gemini: Technical Deep Dive on 1.5 Pro vs. 1.5 Flash]]** — This document provides a detailed technical comparison between Google's two primary Gemini 1.5 models: the powerful, long-context 1.5 Pro and the high-speed, cost-effective 1.5 Flash. It analyzes their performance on latency and cost, their architectural trade-offs, and provides clear implementation logic for developers to choose the right model for tasks ranging from complex video analysis to real-time chatbot responses.
- **[[3_claude|Claude: Technical Deep Dive on Opus vs. Sonnet vs. Haiku]]** — This document provides a detailed technical comparison of Anthropic's Claude model family, breaking down the performance, cost, and architectural differences between the high-power Opus, balanced Sonnet, and high-speed Haiku models. It provides clear implementation logic for developers to select the optimal model for tasks ranging from complex legal analysis to real-time customer support chatbots.
- **[[4_mistral-mixtral|Mistral: Technical Deep Dive on 7B vs. Mixtral vs. Large]]** — This document provides a detailed technical comparison of Mistral AI's model hierarchy, analyzing the architectural differences, performance benchmarks, and cost implications of the efficient Mistral 7B, the powerful Mixture-of-Experts (MoE) Mixtral model, and the flagship proprietary Mistral Large. It concludes with clear implementation logic for developers to select the optimal model based on their needs for speed, power, or advanced reasoning.
- **[[5_llama|Llama 3: Technical Deep Dive on 8B vs. 70B vs. 400B+]]** — This document provides a detailed technical comparison of Meta's Llama 3 model family, analyzing the performance benchmarks, architectural scaling, and resource requirements of the 8B, 70B, and 400B+ parameter versions. It offers clear implementation logic for developers to select the optimal model based on their needs for on-device speed, balanced self-hosted power, or state-of-the-art reasoning.
- **[[6_midjourney|Midjourney: Technical Deep Dive on v6 vs. Niji v6]]** — This document provides a detailed technical comparison between Midjourney's two core models: the general-purpose v6, which excels at photorealism and complex scenes, and the specialized Niji v6, which is fine-tuned for anime and illustrative aesthetics. It analyzes their differences in prompt interpretation, stylistic output, and character coherence, offering clear implementation logic for creative teams to choose the optimal model for tasks ranging from marketing imagery to character design.
- **[[7_stable-diffusion|Stable Diffusion: Technical Deep Dive on SD3 vs. SDXL]]** — This document provides a detailed technical comparison between Stability AI's two flagship open-source image models: the U-Net-based Stable Diffusion XL (SDXL) and the newer Diffusion Transformer (DiT) based Stable Diffusion 3 (SD3). It analyzes their core architectural differences, focusing on SD3's superior prompt adherence, text generation, and multi-subject composition, while acknowledging SDXL's vast ecosystem of fine-tuned models and tools like ControlNet.
- **[[8_code-llama|Code Llama: Technical Deep Dive on Base vs. Python vs. Instruct]]** — This document provides a detailed technical comparison of Meta's specialized Code Llama family. It analyzes the functional differences between the foundational Base model, the specialized Python-tuned model, and the conversational Instruction-tuned model. The note provides clear implementation logic for developers to choose the optimal variant for tasks like code completion, Python-specific development, and natural language-driven code generation.
- **[[9_ollm|oLLM: Technical Deep Dive on SSD Offloading for LLMs]]** — This document provides a detailed technical analysis of oLLM, an open-source Python library. It explains the core mechanism of offloading model weights and the KV cache to fast SSD storage, which enables the execution of very large language models (80B+ parameters) with extensive context on consumer-grade GPUs with low VRAM. The note emphasizes the primary trade-off: sacrificing real-time inference speed for accessibility and the ability to handle massive contexts without expensive hardware.

- **[[claude/index|Claude]]**`
