---
title: "SEO Fundamentals: Core Concepts of Search"
summary: An index of the core principles of SEO, from how search engines work to modern concepts like E-E-A-T and AI's role in search.
seo_category: fundamentals
difficulty: beginner
last_updated: 2025-01-24
kb_status: published
tags:
  - index
  - fundamentals
  - seo-basics
  - overview
---
# Methods


---

## Contents

- **[[01_architectures-and-llms|LLM and System Architectures: From Transformers to Agentic AI]]** — Understanding AI architectures involves looking at two distinct but interconnected layers: the internal model architecture that gives a Large Language Model (LLM) its capabilities, and the external system architecture that integrates the LLM into a functional application.
- **[[02_embeddings-and-vectorization|Embeddings and Vectorization: Translating Meaning into Math]]** — This document provides a foundational overview of embeddings, the numerical vectors that represent complex data like text for AI systems. It details the vectorization pipeline (chunking, embedding, storing, retrieving), key technologies like embedding models and vector databases, and best practices for creating high-quality embeddings to power applications like Retrieval-Augmented Generation (RAG) and semantic search.
- **[[03_training-and-finetuning|AI Model Training and Fine-Tuning: From Foundational Models to Specialists]]** — This document distinguishes between the two primary phases of LLM learning: pre-training, which creates generalist foundational models, and fine-tuning, which specializes them for specific tasks, styles, or domains. It contrasts fine-tuning with Retrieval-Augmented Generation (RAG), explaining that fine-tuning teaches skills while RAG provides knowledge. The note also covers modern, efficient fine-tuning methods like PEFT and LoRA, and alignment techniques such as RLHF and DPO.
- **[[04_prompt-engineering-basics|Prompt Engineering Basics: Crafting Effective Instructions for AI Models]]** — This document provides a foundational guide to prompt engineering, the art of crafting clear instructions to guide AI models. It covers core principles like clarity and specificity, and introduces the critical distinction between prompt engineering (the instructions) and context engineering (the holistic management of the AI's entire context window). The note positions prompt engineering as the first and most important step in AI development, to be mastered before resorting to more complex methods like RAG or fine-tuning.
- **[[05_prompt-frameworks|Prompt Frameworks: Structured Techniques for Enhanced AI Interactions]]** — This document details structured prompt frameworks designed to improve the quality and reliability of LLM outputs. It moves beyond basic prompting to cover methodologies like Chain of Thought (CoT), which encourages step-by-step reasoning, and the PTCF (Prompt, Task, Context, Feedback) framework for segmenting complex queries. These techniques help enhance coherence, ensure consistency, and boost the relevance of AI-generated content.
- **[[06_agentic-ai-overview|Agentic AI Overview: Understanding Autonomous, Goal-Driven Systems]]** — This document defines Agentic AI as a paradigm where autonomous systems perceive, plan, and act to achieve goals. It distinguishes agentic workflows from simple automation by highlighting their strategic purpose: to reduce the 'Human Correction Tax' by shifting the human operator into a 'Fleet Commander' role. The note outlines the core components of an agentic system (reasoning engine, tools, memory, planning) and the cyclical 'agentic loop' (perceive, plan, act, observe) that governs their operation.
- **[[07_multimodal-ai|Advanced Multimodal RAG]]** — An advanced RAG pipeline that improves multimodal responses by creating context-aware image summaries and using the generated text response to guide image selection, overcoming common context loss issues.
- **[[08_evaluation-and-performance|Evaluation and Performance: Assessing the Effectiveness of AI Models]]** — A detailed guide on evaluating AI model performance, focusing on key metrics, methodologies, and best practices used to ensure accuracy, reliability, and efficiency in various AI systems.
- **[[09_advanced-prompt-engineering|Advanced Prompt Engineering for AI and Marketing]]** — Advanced Prompt Engineering is the structured design of queries and instructions that direct AI language models—like ChatGPT, Gemini, or Claude—to produce predictable, relevant, and goal-aligned outputs.
- **[[10_agentic-architectures-and-frameworks|Agentic AI Systems: Architectures, Frameworks, and Memory]]** — This reference document provides a technical overview of the core components required to design, build, and deploy production-ready agentic systems. It covers foundational architectural patterns, leading development frameworks, and advanced memory systems.
- **[[11_agentic-context-engineering|Agentic Context Engineering: Structuring Contexts for Autonomous, Reliable AI Agents]]** — Explore Agentic Context Engineering (ACE), the practice of designing and managing context windows to enhance reasoning within autonomous AI agents. Learn techniques for structuring, compacting, and validating contexts to maintain truthfulness, efficiency, and alignment.
- **[[12_advanced-multimodal-rag.md|Advanced Multimodal RAG]]** — An advanced RAG pipeline that improves multimodal responses by creating context-aware image summaries and using the generated text response to guide image selection, overcoming common context loss issues.
- **[[13_perplexity-search-api|Perplexity Search API]]** — This document provides a technical overview of the Perplexity Search API, which offers real-time, fine-grained web search results for AI applications. It contrasts the raw-data Search API with the conversational Sonar API, highlights features like AI-powered parsing and freshness, and mentions the open-source `search_evals` framework for performance benchmarking.
- **[[14_custom-llm-memory-layer|How to Build a Custom LLM Memory Layer]]** — This document provides a technical walkthrough for creating a custom memory layer for LLMs, addressing their stateless nature. It covers the four key stages: extracting atomic memories from conversations using DSPy, embedding them into a vector DB (QDrant), retrieving relevant memories via tool-calling, and maintaining memory state (add, update, delete) with a ReAct agent. The architecture treats memory as a context engineering problem, enabling personalized and stateful AI agent interactions.
- **[[15_custom-llm-memory-layer|How to Build a Custom LLM Memory Layer]]** — This document provides a technical walkthrough for creating a custom memory layer for LLMs, addressing their stateless nature. It covers four key stages: extracting atomic memories from conversations using DSPy, embedding them into a vector DB (QDrant), retrieving relevant memories via tool-calling, and maintaining memory state (add, update, delete) with a ReAct agent. The architecture treats memory as a context engineering problem, enabling personalized and stateful AI agent interactions.
- **[[16_build-a-self-organizing-agent-memory-system|Build a Self-Organizing Agent Memory System]]** — This document details a Python implementation of a self-organizing memory system for AI agents. It uses SQLite to store 'memory cells' and 'scenes', separating the reasoning logic from memory management. Key features include full-text search, salience scoring, and automated summarization of interaction scenes to enable long-term reasoning without context window bloat.
- **[[17_deep-research-solution-with-microsoft-agent-framework|Deep Research Solution with Microsoft Agent Framework]]** — This guide details the implementation of a Deep Research agent using Microsoft Foundry Local and Agent Framework. It covers the 'research-judge-research' loop, Red Team safety evaluation, DevUI debugging, and performance optimization with .NET Aspire.
- **[[mcp/index|Mcp]]**
